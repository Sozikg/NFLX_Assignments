{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waA-CPPLFP96"
      },
      "source": [
        "# Unit 2 Assessment: Big Data and Spark\n",
        "----\n",
        "\n",
        "### Overview\n",
        "\n",
        "The Unit 2 assessment covers Big Data, Spark SQL, PySpark, PyTest, and Great Expectations.  All of the questions for this assessment are contained in the `Unit_2_Assessment_unsolved.ipynb` Jupyter Notebook file. **The assessment is worth 50 points.**\n",
        "\n",
        "### Files\n",
        "\n",
        "Use the following link to download the assessment instructions and Jupyter Notebook file.\n",
        "\n",
        "[Download the Unit 2 Assessment resources](https://2u-data-curriculum-team.s3.amazonaws.com/nflx-data-science-adv/week-6/Unit_2_Assessment.zip)\n",
        "\n",
        "### Instructions\n",
        "\n",
        "Keep the following mind while working on the assessment: \n",
        "\n",
        "* Remember that this is an individual assessment&mdash;you may not work with your classmates. However, you are free to consult your course notes and activities to help you answer the questions. \n",
        "\n",
        "* Although this assessment is delivered in a Jupyter Notebook, we recommend that make a copy of the `Unit_2_Assessment_unsolved.ipynb` file and upload into Google Colab. \n",
        "\n",
        "    > **Note:** If your answers are not clearly identified, you may receive a score of “0” for that question. \n",
        "\n",
        "* When you are ready to submit your assessment, rename the Google Colab notebook file with your last name. For example, `Unit_2_Assessment_<your_last_name>.ipynb`. Please do not clear your outputs if you have written code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igYv6X7lDipu"
      },
      "source": [
        "## Question 1\n",
        "\n",
        "- **3 points**\n",
        "\n",
        "How do you display the schema of a Spark DataFrame?\n",
        "\n",
        "a. `.showSchema()`\n",
        "\n",
        "b. `.displaySchema()`\n",
        "\n",
        "c. `.printSchema()`\n",
        "\n",
        "d.`.schema().show()`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C) .printSchema()"
      ],
      "metadata": {
        "id": "pJ8JrGOPzYo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm6P_d7FDteo"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "- **3 points**\n",
        "\n",
        "How do you add a new column named, \"half_price\" that is half the price of the \"price\" column in a Spark DataFrame? \n",
        "\n",
        "a. `df.withColumn('half_price',df['price']/2)`\n",
        "\n",
        "b. `df.newColumn('half_price', df['price']/2)`\n",
        "\n",
        "c. `df.Column('half_price', df['price']/2)`\n",
        "\n",
        "d.  `df.withColumn('half_price', ['price']/2)`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A) df.withColumn('half_price',df['price']/2)"
      ],
      "metadata": {
        "id": "YaSqkjRO1DUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6aInLH5D2C4"
      },
      "source": [
        "## Question 3\n",
        "\n",
        "- **3 points**\n",
        "\n",
        "\n",
        "How do you convert a PySpark DataFrame, `df` to a Pandas DataFrame after you use `import pandas as pd`?\n",
        "\n",
        "a. `df = pd.toPandas()`\n",
        "\n",
        "b. `pandas_df = pd.df.toPandas()`\n",
        "\n",
        "c. `pandas_df = df.toPandas()`\n",
        "\n",
        "d. `df = pd.df.toPandas()`\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C) pandas_df = df.toPandas()"
      ],
      "metadata": {
        "id": "AsVuEw6w1WTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rweor-nWEXQX"
      },
      "source": [
        "## Question 4\n",
        "\n",
        "- **8 points**\n",
        "\n",
        "Read in the [new vehicle](https://2u-data-curriculum-team.s3.amazonaws.com/nflx-data-science-adv/week-6/new-vehicles.csv) dataset into Spark DataFrame and create a group by object that shows the total number of the all the vehicles in each \"New_Vehicle_Category\", then answer the following question. \n",
        "\n",
        "**How many passenger cars are there?**\n",
        "\n",
        "  - **Hint:** You will have to change the \"Count\" column to an integer.\n",
        "\n",
        "a. 230,220\n",
        "\n",
        "b. 47,425\n",
        "\n",
        "c. 2,254\n",
        "\n",
        "d. 397,182"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# D) 397,182"
      ],
      "metadata": {
        "id": "zr1-TfIwk7Rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ruAOJ0BbE6dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "049f5d41-f85f-4d69-8725-798b50b7af33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [Waiting for headers] [W\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (185.125.190.36\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (185.125.190.36\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\r                                                                               \rGet:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:10 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,155 kB]\n",
            "Fetched 2,432 kB in 3s (873 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "# Activate Spark in our Colab notebook.\n",
        "import os\n",
        "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example: 'spark-3.2.2'\n",
        "spark_version = 'spark-3.2.2'\n",
        "# spark_version = 'spark-3.<enter version>'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.2.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3.2\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lqCADKk-E9XF"
      },
      "outputs": [],
      "source": [
        "# Start Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Vehicles\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "MVW7jBoKUeve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67a9968a-2b37-4205-bdf6-efbade95a27e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------------+-----------------+----------------+-----+\n",
            "|New_Vehicle_Category|New_Vehicle_Make|New_Vehicle_Model|New_Vehicle_Year|Count|\n",
            "+--------------------+----------------+-----------------+----------------+-----+\n",
            "|1 - Category 1 Truck|           Acura|          RDX 2WD|            2010|   15|\n",
            "|1 - Category 1 Truck|           Acura|          RDX 4WD|            2007|    1|\n",
            "|1 - Category 1 Truck|           Acura|          RDX 4WD|            2008|    2|\n",
            "|1 - Category 1 Truck|           Acura|          RDX 4WD|            2009|  136|\n",
            "|1 - Category 1 Truck|           Acura|          RDX 4WD|            2010|   30|\n",
            "|1 - Category 1 Truck|           Acura|              TSX|            2009|   14|\n",
            "|1 - Category 1 Truck|           Acura|              TSX|            2010|   14|\n",
            "|1 - Category 1 Truck|            Audi|               A3|            2009|    2|\n",
            "|1 - Category 1 Truck|            Audi|       A3 Quattro|            2009|    1|\n",
            "|1 - Category 1 Truck|            Audi|               A4|            2009|    3|\n",
            "|1 - Category 1 Truck|            Audi|               A4|            2010|    1|\n",
            "|1 - Category 1 Truck|            Audi|     A4 Cabriolet|            2009|    2|\n",
            "|1 - Category 1 Truck|            Audi|       A4 Quattro|            2009|    2|\n",
            "|1 - Category 1 Truck|            Audi|       A4 Quattro|            2010|    1|\n",
            "|1 - Category 1 Truck|            Audi|       A5 Quattro|            2010|    1|\n",
            "|1 - Category 1 Truck|            Audi|               Q5|            2009|   46|\n",
            "|1 - Category 1 Truck|            Audi|               Q5|            2010|  191|\n",
            "|1 - Category 1 Truck|            Audi|         UNLISTED|            2010|    1|\n",
            "|1 - Category 1 Truck|             BMW|             128i|            2009|    2|\n",
            "|1 - Category 1 Truck|             BMW|     328ci xDrive|            2009|    1|\n",
            "+--------------------+----------------+-----------------+----------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read in data from S3 Buckets\n",
        "from pyspark import SparkFiles\n",
        "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/nflx-data-science-adv/week-6/new-vehicles.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "vehicle_df = spark.read.csv(SparkFiles.get(\"new-vehicles.csv\"), sep=\",\", header=True)\n",
        "\n",
        "# Show DataFrame\n",
        "vehicle_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vehicle_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rngxohRO2ffQ",
        "outputId": "928c15fc-3e45-4d72-dd25-ca44eb9d7770"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- New_Vehicle_Category: string (nullable = true)\n",
            " |-- New_Vehicle_Make: string (nullable = true)\n",
            " |-- New_Vehicle_Model: string (nullable = true)\n",
            " |-- New_Vehicle_Year: string (nullable = true)\n",
            " |-- Count: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vehicle_df2 = vehicle_df.withColumn(\"Count\",vehicle_df.Count.cast(\"int\"))"
      ],
      "metadata": {
        "id": "lG5DxtMq3Ygd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vehicle_df2.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COPdax0P6cO1",
        "outputId": "8555c5e5-8be2-4bf5-8c73-fdec299be7f1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- New_Vehicle_Category: string (nullable = true)\n",
            " |-- New_Vehicle_Make: string (nullable = true)\n",
            " |-- New_Vehicle_Model: string (nullable = true)\n",
            " |-- New_Vehicle_Year: string (nullable = true)\n",
            " |-- Count: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vehicle_df2.createOrReplaceTempView('new_vehicles')"
      ],
      "metadata": {
        "id": "pBPA3-UXJCCN"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql = \"\"\"\n",
        "SELECT New_Vehicle_Category, sum(Count) FROM new_vehicles\n",
        "Group by 1\n",
        "order by 2 desc\n",
        "\n",
        " \"\"\" \n",
        "\n",
        "spark.sql(sql).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKkvb8FG6mce",
        "outputId": "a25b57bb-873b-4d86-cac9-3121861d94c5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+\n",
            "|New_Vehicle_Category|sum(Count)|\n",
            "+--------------------+----------+\n",
            "|   P - Passenger Car|    397182|\n",
            "|1 - Category 1 Truck|    230220|\n",
            "|2 - Category 2 Truck|     47425|\n",
            "|3 - Category 3 Truck|      2254|\n",
            "+--------------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVgqBMJKEqJp"
      },
      "source": [
        "## Question 5\n",
        "\n",
        "- **5 points**\n",
        "\n",
        "Using the [new vehicle](https://2u-data-curriculum-team.s3.amazonaws.com/nflx-data-science-adv/week-6/new-vehicles.csv) dataset create a temporary view of the Spark DataFrame. Use SQL to return the number of each vehicle in descending order, then answer the following question.\n",
        "\n",
        "Which make, model, and year has the most vehicles?\n",
        "\n",
        "a. 2009 Honda Civic\n",
        "\n",
        "b. 2010 Toyota Camry\n",
        "\n",
        "c. 2010 Toyota Corolla\n",
        "\n",
        "d. 2009 Honda Accord"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C) 2010 Toyota Corolla"
      ],
      "metadata": {
        "id": "3eT61eAAlC6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "uwIhrrWlUevj"
      },
      "outputs": [],
      "source": [
        "# Create a temporary view of the vehicle_df.  \n",
        "vehicle_df2.createOrReplaceTempView('vehicles')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sql = \"\"\"\n",
        "SELECT New_Vehicle_Year, New_Vehicle_Make, New_Vehicle_Model, sum(Count), dense_rank() over (order by sum(Count) desc) rnk\n",
        "FROM vehicles\n",
        "Group by 1,2,3\n",
        "\"\"\"\n",
        "spark.sql(sql).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txETDwAFGWB3",
        "outputId": "e429b90a-400b-4639-cc66-7cb11598a944"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----------------+-----------------+----------+---+\n",
            "|New_Vehicle_Year|New_Vehicle_Make|New_Vehicle_Model|sum(Count)|rnk|\n",
            "+----------------+----------------+-----------------+----------+---+\n",
            "|            2010|          Toyota|          Corolla|     26902|  1|\n",
            "|            2009|           Honda|            Civic|     26712|  2|\n",
            "|            2010|          Toyota|            Camry|     23599|  3|\n",
            "|            2009|            Ford|        Focus FWD|     17533|  4|\n",
            "|            2009|         Hyundai|          Elantra|     17036|  5|\n",
            "|            2009|          Nissan|            Versa|     14845|  6|\n",
            "|            2010|          Toyota|            Prius|     14400|  7|\n",
            "|            2009|           Honda|           Accord|     12148|  8|\n",
            "|            2009|           Honda|              Fit|     12040|  9|\n",
            "|            2009|           Honda|         CR-V 4WD|     11569| 10|\n",
            "|            2009|       Chevrolet|           Cobalt|     10691| 11|\n",
            "|            2009|          Nissan|           Altima|     10224| 12|\n",
            "|            2009|            Ford|       Escape FWD|     10097| 13|\n",
            "|            2009|         Hyundai|           Sonata|      9529| 14|\n",
            "|            2010|            Ford|       Fusion FWD|      8804| 15|\n",
            "|            2009|          Toyota|         RAV4 4WD|      8561| 16|\n",
            "|            2009|       Chevrolet|           Malibu|      8541| 17|\n",
            "|            2009|          Nissan|        Sentra FE|      8467| 18|\n",
            "|            2010|           Mazda|                3|      8386| 19|\n",
            "|            2009|         Hyundai|           Accent|      8312| 20|\n",
            "+----------------+----------------+-----------------+----------+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyqfI6_aPy5i"
      },
      "source": [
        "## Question 6\n",
        "\n",
        "- **6 points**\n",
        "\n",
        "Read in the [USA income distribution](https://2u-data-curriculum-team.s3.amazonaws.com/nflx-data-science-adv/week-6/income_distribution.csv) dataset into Spark DataFrame, create a temporary view, and write a query to return the number of households in descending order where the income level is less than $10,000 for the year 2016, then answer the following question. \n",
        "\n",
        "What state has the most number of households where the income in less than $10,000?\n",
        "\n",
        "  - **Hint:** You will have to cast the \"Percent_of_Total_Household\" column to a float and the \"Number_of_Households\" to an integer. \n",
        "\n",
        "a. New York\n",
        "\n",
        "b. California\n",
        "\n",
        "c. Texas\n",
        "\n",
        "d. Puerto Rico "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# B) California"
      ],
      "metadata": {
        "id": "t0C2AvBIlJFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "HIEN0qoVUevs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d714270-b53e-46e1-80fe-a429aa53e037"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+--------------------+---------------------------+--------------------+\n",
            "|Year|  State|        Income_Level|Percent_of_Total_Households|Number_of_Households|\n",
            "+----+-------+--------------------+---------------------------+--------------------+\n",
            "|2009|Alabama|  $10,000 to $14,999|                       7.42|              134988|\n",
            "|2009|Alabama|  $15,000 to $19,999|                       6.87|              125046|\n",
            "|2009|Alabama|  $20,000 to $24,999|                       6.61|              120178|\n",
            "|2009|Alabama|  $25,000 to $29,999|                       6.07|              110437|\n",
            "|2009|Alabama|  $30,000 to $34,999|                       5.78|              105216|\n",
            "|2009|Alabama|  $35,000 to $39,999|                       5.36|               97549|\n",
            "|2009|Alabama|  $40,000 to $44,999|                       5.02|               91415|\n",
            "|2009|Alabama|  $45,000 to $49,999|                       4.38|               79708|\n",
            "|2009|Alabama|  $50,000 to $59,999|                       8.22|              149589|\n",
            "|2009|Alabama|  $60,000 to $74,999|                       9.37|              170463|\n",
            "|2009|Alabama|  $75,000 to $99,999|                      10.51|              191272|\n",
            "|2009|Alabama|$100,000 to $124,999|                       5.96|              108519|\n",
            "|2009|Alabama|$125,000 to $149,999|                       3.05|               55565|\n",
            "|2009|Alabama|$150,000 to $199,999|                       2.53|               46035|\n",
            "|2009|Alabama|    $200,000 or more|                        2.2|               39992|\n",
            "|2009|Alabama|   Less than $10,000|                      10.63|              193469|\n",
            "|2009| Alaska|  $10,000 to $14,999|                       3.84|                9008|\n",
            "|2009| Alaska|  $15,000 to $19,999|                       3.55|                8340|\n",
            "|2009| Alaska|  $20,000 to $24,999|                       4.22|                9918|\n",
            "|2009| Alaska|  $25,000 to $29,999|                       4.41|               10358|\n",
            "+----+-------+--------------------+---------------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkFiles\n",
        "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/nflx-data-science-adv/week-6/income_distribution.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "income_df = spark.read.csv(SparkFiles.get(\"income_distribution.csv\"), sep=\",\", header=True)\n",
        "\n",
        "# Show DataFrame\n",
        "income_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "xffTbxR2K0XR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1717825-d9a6-4071-ab63-ea7aa692f82d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Year: string (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            " |-- Income_Level: string (nullable = true)\n",
            " |-- Percent_of_Total_Households: string (nullable = true)\n",
            " |-- Number_of_Households: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "income_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_cast= [\"Percent_of_Total_Households\", \"Number_of_Households\"]"
      ],
      "metadata": {
        "id": "gD1RbXREK0DC"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "income_df2 = income_df.select(*(c for c in income_df.columns if c not in columns_to_cast), income_df[\"Percent_of_Total_Households\"].cast(\"float\"), income_df[\"Number_of_Households\"].cast(\"int\"))"
      ],
      "metadata": {
        "id": "7mt9lT7MYc0c"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Oq7c85HFK7EM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91b526fc-c50d-44ec-b0e3-53a09007cd84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Year: string (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            " |-- Income_Level: string (nullable = true)\n",
            " |-- Percent_of_Total_Households: float (nullable = true)\n",
            " |-- Number_of_Households: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Convert the Percent_of_Total_Households to a float and Number_of_Households to an integer.\n",
        "income_df2.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "income_df2.createOrReplaceTempView('income_dist') "
      ],
      "metadata": {
        "id": "LKPkwrJCajuR"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql = \"\"\"\n",
        "SELECT state, Year, Income_Level, Number_of_Households, dense_rank() over (order by Number_of_Households desc) as rnk FROM income_dist\n",
        "WHERE Income_Level = 'Less than $10,000' and Year = 2016\n",
        "\n",
        "\"\"\"\n",
        "spark.sql(sql).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feqbPyHZa5Ci",
        "outputId": "a8f5c4bb-70cf-4eee-8c65-e50456fba676"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+----+-----------------+--------------------+---+\n",
            "|         state|Year|     Income_Level|Number_of_Households|rnk|\n",
            "+--------------+----+-----------------+--------------------+---+\n",
            "|    California|2016|Less than $10,000|              728895|  1|\n",
            "|         Texas|2016|Less than $10,000|              644199|  2|\n",
            "|      New York|2016|Less than $10,000|              556825|  3|\n",
            "|       Florida|2016|Less than $10,000|              556637|  4|\n",
            "|          Ohio|2016|Less than $10,000|              362790|  5|\n",
            "|   Puerto Rico|2016|Less than $10,000|              355712|  6|\n",
            "|      Illinois|2016|Less than $10,000|              341280|  7|\n",
            "|  Pennsylvania|2016|Less than $10,000|              334227|  8|\n",
            "|      Michigan|2016|Less than $10,000|              299722|  9|\n",
            "|       Georgia|2016|Less than $10,000|              298701| 10|\n",
            "|North Carolina|2016|Less than $10,000|              294399| 11|\n",
            "|     Tennessee|2016|Less than $10,000|              208317| 12|\n",
            "|       Arizona|2016|Less than $10,000|              183573| 13|\n",
            "|       Alabama|2016|Less than $10,000|              179345| 14|\n",
            "|      Missouri|2016|Less than $10,000|              178563| 15|\n",
            "|       Indiana|2016|Less than $10,000|              176307| 16|\n",
            "|      Virginia|2016|Less than $10,000|              175989| 17|\n",
            "|     Louisiana|2016|Less than $10,000|              175389| 18|\n",
            "|    New Jersey|2016|Less than $10,000|              174835| 19|\n",
            "|      Kentucky|2016|Less than $10,000|              166777| 20|\n",
            "+--------------+----+-----------------+--------------------+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq-kfT6yUewG"
      },
      "source": [
        "## Question 7. \n",
        "\n",
        "- **3 points**\n",
        "\n",
        "If you have a dataset of 1 Tb what is the best option to increase query execution time? \n",
        "\n",
        "a. Create a temporary view, cache the temporary view and rerun the query. \n",
        "\n",
        "b. Write the data to a parquet format and rerun the query. \n",
        "\n",
        "c. Create a temporary view and rerun the query.\n",
        "\n",
        "d. Store in a AWS S3 bucket and query the table on Googl Colab."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# B) Write the data to a parquet format and rerun the query."
      ],
      "metadata": {
        "id": "vRvD8_vRfhR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaZ0jrCSaMr-"
      },
      "source": [
        "## Question 8\n",
        "\n",
        "- **3 points**\n",
        "\n",
        "How do you check that a temporary view table. called, \"flight_delays\", is cached? \n",
        "\n",
        "a. `spark.temporaryView.isCached(\"flight_delays\")`\n",
        "\n",
        "b. `spark.temporaryView()(\"flight_delays\").isCached()`\n",
        "\n",
        "c. `spark.catalog.isCached(\"flight_delays\")`\n",
        "\n",
        "d. `spark.catalog.temporaryView(\"flight_delays\").isCached()`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C) spark.catalog.isCached(\"flight_delays\")"
      ],
      "metadata": {
        "id": "-g6wXyboeF-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO2MMkR1Uewm"
      },
      "source": [
        "## Question 9\n",
        "\n",
        "- **8 points**\n",
        "\n",
        "Read in the [SP_500_5yr_stock_data.csv](\"https://2u-data-curriculum-team.s3.amazonaws.com/nflx-data-science-adv/week-6/SP_500_5yr_stock_data.csv) dataset into Pandas DataFrame. \n",
        "\n",
        "What is the assert statement will pass for the 1,258 rows in the DataFrame? \n",
        "\n",
        "a. `assert len(df.count()) == 1258`\n",
        "\n",
        "b. `assert len(df.index()) == 1258`\n",
        "\n",
        "c. `assert len(df.index) == 1258`\n",
        "\n",
        "d. `assert len(df.count) == 1258`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C) assert len(df.index) == 1258"
      ],
      "metadata": {
        "id": "LrK1jT6bjlUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ByBCXLBAhX00"
      },
      "outputs": [],
      "source": [
        "# Install pytest and pytest-sugar to make our output look nice.\n",
        "!pip install -q pytest pytest-sugar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "8Hjv3hBphemZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "93b58b82-4000-44ee-df00-46ee706d28e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tests\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/tests'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "# Create and navigate to the tests directory.\n",
        "from pathlib import Path\n",
        "if Path.cwd().name != 'tests':\n",
        "    %mkdir tests\n",
        "    %cd tests\n",
        "# Show the current working directory. \n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "cgEpdizPhjy0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4238f87-4ea3-41c0-8beb-f973e54e8dd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing __init__.py\n"
          ]
        }
      ],
      "source": [
        "# Create a  __init__.py file that will contain that will be used to run our functions. \n",
        "# This file will be stored in our pwd (/content/tests)\n",
        "%%file __init__.py\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Nc0JGQHody5D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1530e66-951d-4dfd-c124-14798f2b1b40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing stock_data.py\n"
          ]
        }
      ],
      "source": [
        "# Create a bank_data.py file that will contain the import_data function. \n",
        "# This file will be stored in our pwd (/content/tests).\n",
        "%%file stock_data.py\n",
        "    \n",
        "import pandas as pd\n",
        "\n",
        "def import_data():\n",
        "  url = \"https://2u-data-curriculum-team.s3.amazonaws.com/nflx-data-science-adv/week-6/SP_500_5yr_stock_data.csv\"\n",
        "  df = pd.read_csv(url)\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "S7jfvswveA5W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2d5a05b-8893-4bff-e7e9-96ee0ebc4406"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_stock_data.py\n"
          ]
        }
      ],
      "source": [
        "%%file test_stock_data.py\n",
        "from stock_data import import_data\n",
        "def test_row_count():\n",
        "  df = import_data()\n",
        "  assert len(df.index) == 1258\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "cEdynwDbiO8h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a653e42-9109-4d9c-e64b-9d19ca0873ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mTest session starts (platform: linux, Python 3.7.14, pytest 3.6.4, pytest-sugar 0.9.5)\u001b[0m\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1, sugar-0.9.5\n",
            "\n",
            " \u001b[36m\u001b[0mtest_stock_data.py\u001b[0m \u001b[32m✓\u001b[0m                                            \u001b[32m100% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█████████\u001b[0m\n",
            "\n",
            "Results (0.91s):\n",
            "\u001b[32m       1 passed\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m pytest test_stock_data.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ih5d0yBaWg8"
      },
      "source": [
        "## Question 10\n",
        "\n",
        "- **8 points**\n",
        "\n",
        "Read in the [SP_500_5yr_stock_data.csv](\"https://2u-data-curriculum-team.s3.amazonaws.com/nflx-data-science-adv/week-6/SP_500_5yr_stock_data.csv) dataset into Spark DataFrame called, `stock_df`, then import the Spark DataFrame into a Great Expectations DataFrame called, `stock_df_ge = ge.dataset.SparkDFDataset(stock_df)`, and answer the following question:\n",
        "\n",
        "What is the code that will test that the number of columns is \"6\" and pass when executed?\n",
        "\n",
        "a. `assert stock_df_ge.column_count() == 6`\n",
        "\n",
        "b. `print(stock_df_ge.expect_column_count_to_equal(6))`\n",
        "\n",
        "c. `print(stock_df_ge.expect_column_count_to_equal(6))`\n",
        "\n",
        "d. `print(stock_df_ge.expect_table_column_count_to_equal(6))`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# D) print(stock_df_ge.expect_table_column_count_to_equal(6))"
      ],
      "metadata": {
        "id": "Nd7FTaXCm8uC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6D8AaFO4n1oh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "748c4357-3749-4bbf-fb7b-13e95a46e761"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: great_expectations in /usr/local/lib/python3.7/dist-packages (0.15.26)\n",
            "Requirement already satisfied: pyparsing>=2.4 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (3.0.9)\n",
            "Requirement already satisfied: jinja2>=2.10 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (2.11.3)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (1.3.5)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (1.7.3)\n",
            "Requirement already satisfied: tqdm>=4.59.0 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (4.64.1)\n",
            "Requirement already satisfied: Ipython>=7.16.3 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (7.34.0)\n",
            "Requirement already satisfied: Click>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (7.1.2)\n",
            "Requirement already satisfied: mistune>=0.8.4 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (0.8.4)\n",
            "Requirement already satisfied: altair<5,>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (4.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (2.8.2)\n",
            "Requirement already satisfied: jsonschema>=2.5.1 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (4.3.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from great_expectations) (21.3)\n",
            "Requirement already satisfied: cryptography>=3.2 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (38.0.1)\n",
            "Requirement already satisfied: ipywidgets>=7.5.1 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (7.7.1)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (1.5.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (1.1.0)\n",
            "Requirement already satisfied: colorama>=0.4.3 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (0.4.5)\n",
            "Requirement already satisfied: importlib-metadata>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (4.12.0)\n",
            "Requirement already satisfied: nbformat>=5.0 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (4.1.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (2.23.0)\n",
            "Requirement already satisfied: pytz>=2021.3 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (2022.2.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.7.1 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (3.17.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (1.25.11)\n",
            "Requirement already satisfied: makefun<2,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (1.15.0)\n",
            "Requirement already satisfied: jsonpatch>=1.22 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (1.32)\n",
            "Requirement already satisfied: ruamel.yaml<0.17.18,>=0.16 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (0.17.17)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (1.21.6)\n",
            "Requirement already satisfied: notebook>=6.4.10 in /usr/local/lib/python3.7/dist-packages (from great_expectations) (6.4.12)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair<5,>=4.0.0->great_expectations) (0.12.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair<5,>=4.0.0->great_expectations) (0.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=3.2->great_expectations) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=3.2->great_expectations) (2.21)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.7.0->great_expectations) (3.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from Ipython>=7.16.3->great_expectations) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from Ipython>=7.16.3->great_expectations) (4.4.2)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from Ipython>=7.16.3->great_expectations) (0.1.6)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from Ipython>=7.16.3->great_expectations) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from Ipython>=7.16.3->great_expectations) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from Ipython>=7.16.3->great_expectations) (0.18.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from Ipython>=7.16.3->great_expectations) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from Ipython>=7.16.3->great_expectations) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from Ipython>=7.16.3->great_expectations) (2.0.10)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from Ipython>=7.16.3->great_expectations) (57.4.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->great_expectations) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->great_expectations) (3.6.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->great_expectations) (5.3.4)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->great_expectations) (3.0.3)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->great_expectations) (6.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->great_expectations) (6.1.12)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->Ipython>=7.16.3->great_expectations) (0.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.10->great_expectations) (2.0.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.7/dist-packages (from jsonpatch>=1.22->great_expectations) (2.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.5.1->great_expectations) (0.18.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.5.1->great_expectations) (22.1.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.5.1->great_expectations) (5.9.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=5.0->great_expectations) (2.16.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=5.0->great_expectations) (4.11.1)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.7/dist-packages (from notebook>=6.4.10->great_expectations) (1.5.6)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from notebook>=6.4.10->great_expectations) (23.2.1)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.7/dist-packages (from notebook>=6.4.10->great_expectations) (5.6.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from notebook>=6.4.10->great_expectations) (0.14.1)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from notebook>=6.4.10->great_expectations) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from notebook>=6.4.10->great_expectations) (0.13.3)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.7/dist-packages (from notebook>=6.4.10->great_expectations) (21.3.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert>=5->notebook>=6.4.10->great_expectations) (5.0.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=5->notebook>=6.4.10->great_expectations) (1.5.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert>=5->notebook>=6.4.10->great_expectations) (0.6.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert>=5->notebook>=6.4.10->great_expectations) (0.7.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->Ipython>=7.16.3->great_expectations) (0.7.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->Ipython>=7.16.3->great_expectations) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->Ipython>=7.16.3->great_expectations) (0.2.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->great_expectations) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->great_expectations) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->great_expectations) (2022.6.15)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.1.2 in /usr/local/lib/python3.7/dist-packages (from ruamel.yaml<0.17.18,>=0.16->great_expectations) (0.2.6)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.7/dist-packages (from argon2-cffi->notebook>=6.4.10->great_expectations) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert>=5->notebook>=6.4.10->great_expectations) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "# Install great expectations\n",
        "!pip install great_expectations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "srB4N3ZaqBCQ"
      },
      "outputs": [],
      "source": [
        "# Start Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "import great_expectations as ge\n",
        "spark = SparkSession.builder.appName(\"Vehicles\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "FhLmF4qMrxg7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1b8e6ef-47d3-47fa-fe38-2daeaaca49b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+-------+-------+-------+---------+\n",
            "|      Date|   Open|   High|    Low|  Close|   Volume|\n",
            "+----------+-------+-------+-------+-------+---------+\n",
            "|01/04/2016| 2038.2| 2038.2|1989.68|2012.66|802072115|\n",
            "|01/05/2016|2013.78|2021.94|2004.17|2016.71|619260483|\n",
            "|01/06/2016|2011.71|2011.71|1979.05|1990.26|734820348|\n",
            "|01/07/2016|1985.32|1985.32|1938.83|1943.09|860517477|\n",
            "|01/08/2016|1945.97| 1960.4|1918.46|1922.03|800798104|\n",
            "|01/11/2016|1926.12|1935.65| 1901.1|1923.67|775646469|\n",
            "|01/12/2016|1927.83|1947.38|1914.35|1938.68|759189614|\n",
            "|01/13/2016|1940.34|1950.33|1886.41|1890.28|874565406|\n",
            "|01/14/2016|1891.68|1934.47|1878.93|1921.84|920305719|\n",
            "|01/15/2016|1916.68|1916.68|1857.83|1880.33|121200428|\n",
            "|01/19/2016|1888.66|1901.44| 1864.6|1881.33|876359950|\n",
            "|01/20/2016|1876.18|1876.18|1812.29|1859.33|  4309713|\n",
            "|01/21/2016|1861.46|1889.85|1848.98|1868.99|853633943|\n",
            "|01/22/2016| 1877.4|1908.85| 1877.4| 1906.9|809147435|\n",
            "|01/25/2016|1906.28|1906.28|1875.97|1877.08|760931974|\n",
            "|01/26/2016|1878.79|1906.73|1878.79|1903.63|695956044|\n",
            "|01/27/2016|1902.52|1916.99| 1872.7|1882.95|801306427|\n",
            "|01/28/2016|1885.22|1902.96|1873.65|1893.36|820296447|\n",
            "|01/29/2016| 1894.0|1940.24| 1894.0|1940.24|144609311|\n",
            "|02/01/2016|1936.94| 1947.2| 1920.3|1939.38|714148314|\n",
            "+----------+-------+-------+-------+-------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkFiles\n",
        "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/nflx-data-science-adv/week-6/SP_500_5yr_stock_data.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "stock_df = spark.read.csv(SparkFiles.get(\"SP_500_5yr_stock_data.csv\"), header=True)\n",
        "\n",
        "# Show DataFrame\n",
        "stock_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the Spark DataFrame into a Great Expectations DataFrame. \n",
        "stock_df_ge = ge.dataset.SparkDFDataset(stock_df)"
      ],
      "metadata": {
        "id": "3aE5DNWGmMnu"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_df_ge.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "PzHrBMnvmVHF",
        "outputId": "05ec6b32-cc17-4c03-9029-c817c26f0234"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Date     Open     High      Low    Close     Volume\n",
              "0  01/04/2016   2038.2   2038.2  1989.68  2012.66  802072115\n",
              "1  01/05/2016  2013.78  2021.94  2004.17  2016.71  619260483\n",
              "2  01/06/2016  2011.71  2011.71  1979.05  1990.26  734820348\n",
              "3  01/07/2016  1985.32  1985.32  1938.83  1943.09  860517477\n",
              "4  01/08/2016  1945.97   1960.4  1918.46  1922.03  800798104"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-737431dc-6e54-4131-a8df-3bf371e469c7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>01/04/2016</td>\n",
              "      <td>2038.2</td>\n",
              "      <td>2038.2</td>\n",
              "      <td>1989.68</td>\n",
              "      <td>2012.66</td>\n",
              "      <td>802072115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>01/05/2016</td>\n",
              "      <td>2013.78</td>\n",
              "      <td>2021.94</td>\n",
              "      <td>2004.17</td>\n",
              "      <td>2016.71</td>\n",
              "      <td>619260483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>01/06/2016</td>\n",
              "      <td>2011.71</td>\n",
              "      <td>2011.71</td>\n",
              "      <td>1979.05</td>\n",
              "      <td>1990.26</td>\n",
              "      <td>734820348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>01/07/2016</td>\n",
              "      <td>1985.32</td>\n",
              "      <td>1985.32</td>\n",
              "      <td>1938.83</td>\n",
              "      <td>1943.09</td>\n",
              "      <td>860517477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>01/08/2016</td>\n",
              "      <td>1945.97</td>\n",
              "      <td>1960.4</td>\n",
              "      <td>1918.46</td>\n",
              "      <td>1922.03</td>\n",
              "      <td>800798104</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-737431dc-6e54-4131-a8df-3bf371e469c7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-737431dc-6e54-4131-a8df-3bf371e469c7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-737431dc-6e54-4131-a8df-3bf371e469c7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "SVwnLtY8tAC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b050cee-bcbe-4326-afea-a54b33ca496c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"result\": {\n",
            "    \"observed_value\": 6\n",
            "  },\n",
            "  \"expectation_config\": {\n",
            "    \"expectation_type\": \"expect_table_column_count_to_equal\",\n",
            "    \"kwargs\": {\n",
            "      \"value\": 6,\n",
            "      \"result_format\": \"BASIC\"\n",
            "    },\n",
            "    \"meta\": {}\n",
            "  },\n",
            "  \"success\": true,\n",
            "  \"meta\": {},\n",
            "  \"exception_info\": {\n",
            "    \"raised_exception\": false,\n",
            "    \"exception_traceback\": null,\n",
            "    \"exception_message\": null\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Test that the number of columns is \"6\" and passes when executed.\n",
        "print(stock_df_ge.expect_table_column_count_to_equal(6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MqJl3i5tHcI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('qatest')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "nbpresent": {
      "slides": {},
      "themes": {
        "default": "0535adbc-b74f-46cc-9cd6-4eabe2477c8e",
        "theme": {
          "0535adbc-b74f-46cc-9cd6-4eabe2477c8e": {
            "backgrounds": {
              "backgroundColor": {
                "background-color": "backgroundColor",
                "id": "backgroundColor"
              }
            },
            "id": "0535adbc-b74f-46cc-9cd6-4eabe2477c8e",
            "palette": {
              "backgroundColor": {
                "id": "backgroundColor",
                "rgb": [
                  43,
                  43,
                  43
                ]
              },
              "headingColor": {
                "id": "headingColor",
                "rgb": [
                  238,
                  238,
                  238
                ]
              },
              "linkColor": {
                "id": "linkColor",
                "rgb": [
                  19,
                  218,
                  236
                ]
              },
              "mainColor": {
                "id": "mainColor",
                "rgb": [
                  238,
                  238,
                  238
                ]
              }
            },
            "rules": {
              "a": {
                "color": "linkColor"
              },
              "h1": {
                "color": "headingColor",
                "font-family": "Oswald",
                "font-size": 7
              },
              "h2": {
                "color": "headingColor",
                "font-family": "Oswald",
                "font-size": 5
              },
              "h3": {
                "color": "headingColor",
                "font-family": "Oswald",
                "font-size": 3.75
              },
              "h4": {
                "color": "headingColor",
                "font-family": "Oswald",
                "font-size": 3
              },
              "h5": {
                "color": "headingColor",
                "font-family": "Oswald"
              },
              "h6": {
                "color": "headingColor",
                "font-family": "Oswald"
              },
              "h7": {
                "color": "headingColor",
                "font-family": "Oswald"
              },
              "li": {
                "color": "mainColor",
                "font-family": "Lato",
                "font-size": 5
              },
              "p": {
                "color": "mainColor",
                "font-family": "Lato",
                "font-size": 5
              }
            },
            "text-base": {
              "color": "mainColor",
              "font-family": "Lato",
              "font-size": 5
            }
          },
          "cc59980f-cb69-400a-b63a-1fb85ca73c8a": {
            "backgrounds": {
              "dc7afa04-bf90-40b1-82a5-726e3cff5267": {
                "background-color": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
                "id": "dc7afa04-bf90-40b1-82a5-726e3cff5267"
              }
            },
            "id": "cc59980f-cb69-400a-b63a-1fb85ca73c8a",
            "palette": {
              "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
                "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
                "rgb": [
                  252,
                  252,
                  252
                ]
              },
              "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
                "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
                "rgb": [
                  68,
                  68,
                  68
                ]
              },
              "50f92c45-a630-455b-aec3-788680ec7410": {
                "id": "50f92c45-a630-455b-aec3-788680ec7410",
                "rgb": [
                  197,
                  226,
                  245
                ]
              },
              "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
                "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
                "rgb": [
                  43,
                  126,
                  184
                ]
              },
              "efa7f048-9acb-414c-8b04-a26811511a21": {
                "id": "efa7f048-9acb-414c-8b04-a26811511a21",
                "rgb": [
                  25.118061674008803,
                  73.60176211453744,
                  107.4819383259912
                ]
              }
            },
            "rules": {
              "a": {
                "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c"
              },
              "blockquote": {
                "color": "50f92c45-a630-455b-aec3-788680ec7410",
                "font-size": 3
              },
              "code": {
                "font-family": "Anonymous Pro"
              },
              "h1": {
                "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
                "font-family": "Merriweather",
                "font-size": 8
              },
              "h2": {
                "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
                "font-family": "Merriweather",
                "font-size": 6
              },
              "h3": {
                "color": "50f92c45-a630-455b-aec3-788680ec7410",
                "font-family": "Lato",
                "font-size": 5.5
              },
              "h4": {
                "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
                "font-family": "Lato",
                "font-size": 5
              },
              "h5": {
                "font-family": "Lato"
              },
              "h6": {
                "font-family": "Lato"
              },
              "h7": {
                "font-family": "Lato"
              },
              "li": {
                "color": "50f92c45-a630-455b-aec3-788680ec7410",
                "font-size": 3.25
              },
              "pre": {
                "font-family": "Anonymous Pro",
                "font-size": 4
              }
            },
            "text-base": {
              "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
              "font-family": "Lato",
              "font-size": 4
            }
          }
        }
      }
    },
    "nteract": {
      "version": "0.10.0"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "df07ec09c4017cff5cffe5c6e95982b3fd889b25c9af2c66175cc1127f95fbfd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}